{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71b589f5-ae5c-473b-bfd6-5de75738a9bd",
      "metadata": {
        "id": "71b589f5-ae5c-473b-bfd6-5de75738a9bd"
      },
      "source": [
        "### 1. å¼•å…¥åº“"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "226c5ba7-2267-4477-8f54-e55e8bc6285d",
      "metadata": {
        "id": "226c5ba7-2267-4477-8f54-e55e8bc6285d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t66b6czfrQKC",
      "metadata": {
        "id": "t66b6czfrQKC"
      },
      "source": [
        "### 2. é€‰æ‹©è®¾å¤‡ï¼ˆé€šå¸¸æ˜¯GPUï¼‰\n",
        "`torch.cuda.is_availabel()`è¿”å›ä¸€ä¸ªå¸ƒå°”å€¼ï¼ˆ`True`æˆ–`False`ï¼‰ï¼Œæ˜¯å¦æ”¯æŒCUDAã€‚  \n",
        "ä½¿ç”¨CUDAå¯ä»¥æ˜¾è‘—æé«˜å¹¶è¡Œè®¡ç®—é€Ÿåº¦ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a5053aa-bdd5-4d54-8c5d-53ca3c47fc58",
      "metadata": {
        "id": "6a5053aa-bdd5-4d54-8c5d-53ca3c47fc58"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VUbX5DoqtevJ",
      "metadata": {
        "id": "VUbX5DoqtevJ"
      },
      "source": [
        "### 3. åŠ è½½æ•°æ®é›†"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eDSiCb0N0LvO",
      "metadata": {
        "id": "eDSiCb0N0LvO"
      },
      "source": [
        "#### 3.1 å®šä¹‰è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ•°æ®åŠ è½½å™¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QVlDE3CZtdn7",
      "metadata": {
        "id": "QVlDE3CZtdn7"
      },
      "outputs": [],
      "source": [
        "def get_train_val_loader(data_dir, batch_size, augment,\n",
        "                         random_seed, valid_size = 0.1, shuffle = True):\n",
        "\n",
        "  # ------------- è®¾ç½®å›¾åƒå˜æ¢ ------------- #\n",
        "  # (1) å½’ä¸€åŒ–\n",
        "  normalize = transforms.Normalize(mean = [0.4914, 0.4822, 0.4465],\n",
        "                                   std = [0.2023, 0.1994, 0.2010])\n",
        "  # (2) éªŒè¯é›†å›¾åƒå˜æ¢\n",
        "  val_transform = transforms.Compose([transforms.Resize(227),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   normalize])\n",
        "  # (3) è®­ç»ƒé›†æ˜¯å¦æ•°æ®å¢å¼º\n",
        "  if augment:\n",
        "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.Resize(227),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      normalize])\n",
        "  else:\n",
        "    train_transform = transforms.Compose([transforms.Resize(227),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       normalize])\n",
        "  # ---------- ğŸ‘† æ•°æ®å˜æ¢è®¾ç½®å®Œæ¯• -------------- #\n",
        "\n",
        "  # ä¸‹è½½å¹¶åŠ è½½è®­ç»ƒé›†\n",
        "  train_dataset = datasets.CIFAR10(root = data_dir,\n",
        "                                  train = True,\n",
        "                                  download = True,\n",
        "                                  transform = train_transform)\n",
        "  val_dataset = datasets.CIFAR10(root = data_dir,\n",
        "                                  train = True,\n",
        "                                  download = True,\n",
        "                                  transform = val_transform)\n",
        "\n",
        "  # ---------- åˆ’åˆ†éªŒè¯é›†å’Œè®­ç»ƒé›† ----------  #\n",
        "  # (1) è®¡ç®—è®­ç»ƒé›†å›¾ç‰‡æ•°é‡\n",
        "  num_train = len(train_dataset)\n",
        "  # (2) è®¡ç®—éªŒè¯é›†æ•°é‡ï¼Œå¹¶å‘ä¸‹å–æ•´\n",
        "  num_val = np.floor(valid_size * num_train)\n",
        "  # (3) è®¾ç½®è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ’åˆ†ç•Œé™\n",
        "  split = int(num_val)\n",
        "  # (4) ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨ç´¢å¼•ï¼Œå…¶å†…å®¹ä¸º 0 ~ (num_train - 1) çš„å…¨éƒ¨æ•´æ•°\n",
        "  indices = list(range(num_train))    # ä¸ºæ•°æ®\"æ´—ç‰Œ\"åšå‡†å¤‡\n",
        "  if shuffle:\n",
        "    np.random.seed(random_seed) # æ ¹æ®ç§å­ç”Ÿæˆéšæœºæ•°\n",
        "    np.random.shuffle(indices)  # æ ¹æ®éšæœºæ•°æ‰“ä¹±å›¾ç‰‡\n",
        "  # (5) åˆ’åˆ†éªŒè¯é›†å’Œè®­ç»ƒé›†(æ ¹æ®ç´¢å¼•åˆ—è¡¨ indices å’Œåˆ’åˆ†ç•Œé™ split åˆ’åˆ†)\n",
        "  val_idx = indices[:split]     # éªŒè¯é›†ç´¢å¼•åˆ—åˆ«\n",
        "  train_idx = indices[split:]   # è®­ç»ƒé›†ç´¢å¼•åˆ—è¡¨\n",
        "  # (6) æ ¹æ®éªŒè¯é›†å’Œè®­ç»ƒé›†çš„ç´¢å¼•åˆ—è¡¨é‡‡æ ·æ•°æ®\n",
        "  train_sampler = SubsetRandomSampler(train_idx)\n",
        "  val_sampler = SubsetRandomSampler(val_idx)\n",
        "  # ---------- ğŸ‘† è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ’åˆ†å®Œæ¯• ---------- #\n",
        "\n",
        "  # è®¾ç½®æ•°æ®åŠ è½½å™¨\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                             batch_size = batch_size,\n",
        "                                             sampler = train_sampler)\n",
        "  val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           sampler = val_sampler)\n",
        "\n",
        "  return (train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DQoIBdQMeHiP",
      "metadata": {
        "id": "DQoIBdQMeHiP"
      },
      "source": [
        "åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…æåˆ°äº†æœ¬åœ°å½’ä¸€åŒ–ã€‚`mean = [0.4914, 0.4822, 0.4465], ` `std = [0.2023, 0.1994, 0.2010]` çœ‹ä¸æ‡‚ä¸ç”¨æ‹…å¿ƒã€‚æ·±åº¦å­¦ä¹ ä»¥åå°±ä¸è¿™ä¹ˆç”¨äº†ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Modh5qR4ho3K",
      "metadata": {
        "id": "Modh5qR4ho3K"
      },
      "source": [
        "#### 3.2 å®šä¹‰è·å–æµ‹è¯•é›†çš„æ•°æ®åŠ è½½å™¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BhEZd20Chyjr",
      "metadata": {
        "id": "BhEZd20Chyjr"
      },
      "outputs": [],
      "source": [
        "def get_test_loader(data_dir, batch_size, shuffle = True):\n",
        "\n",
        "  # ------------- è®¾ç½®å›¾åƒå˜æ¢ ------------- #\n",
        "  # å½’ä¸€åŒ–\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225],)\n",
        "  # å›¾åƒå˜æ¢\n",
        "  test_transform = transforms.Compose([transforms.Resize(227),\n",
        "                                transforms.ToTensor(),\n",
        "                                normalize])\n",
        "\n",
        "  # ä¸‹è½½å¹¶åŠ è½½æµ‹è¯•é›†\n",
        "  test_dataset = datasets.CIFAR10(root = data_dir,\n",
        "                                  train = False,\n",
        "                                  download = True,\n",
        "                                  transform = test_transform)\n",
        "\n",
        "  # åŠ è½½æµ‹è¯•æ•°æ®\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                            batch_size = batch_size,\n",
        "                                            shuffle = shuffle)\n",
        "\n",
        "  return test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NpWy6lRor1VM",
      "metadata": {
        "id": "NpWy6lRor1VM"
      },
      "source": [
        "#### 3.3 è°ƒç”¨å‡½æ•°ï¼ŒåŠ è½½æ•°æ®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2-hD9JSfsPg0",
      "metadata": {
        "id": "2-hD9JSfsPg0"
      },
      "outputs": [],
      "source": [
        "# è®¾ç½®æ•°æ®é›†ä¸‹è½½è·¯å¾„\n",
        "data_dir = \"./data\"\n",
        "# è®¾ç½®æ‰¹å°ºå¯¸\n",
        "batch_size = 64\n",
        "# è°ƒç”¨è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ DataLoader\n",
        "train_loader, val_loader = get_train_val_loader(data_dir = data_dir,\n",
        "                                                batch_size = batch_size,\n",
        "                                                augment = True,\n",
        "                                                random_seed = 1)\n",
        "# è°ƒç”¨æµ‹è¯•é›†çš„ DataLoader\n",
        "test_loader = get_test_loader(data_dir = data_dir,\n",
        "                              batch_size = batch_size,\n",
        "                              shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uyxOSRg9twvK",
      "metadata": {
        "id": "uyxOSRg9twvK"
      },
      "source": [
        "### 4. æ„å»ºAlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N83bsw7Et7vs",
      "metadata": {
        "id": "N83bsw7Et7vs"
      },
      "outputs": [],
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "                        nn.BatchNorm2d(96),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "                        nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "                        nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(384),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "        self.conv_block4 = nn.Sequential(\n",
        "                        nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(384),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "        self.conv_block5 = nn.Sequential(\n",
        "                        nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(256),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "        self.pool = nn.MaxPool2d(kernel_size = 3, stride = 2)\n",
        "        self.fc1 = nn.Sequential(nn.Dropout(0.5),nn.Linear(9216, 4096),nn.ReLU())\n",
        "        self.fc2 = nn.Sequential(nn.Dropout(0.5),nn.Linear(4096, 4096),nn.ReLU())\n",
        "        self.fc3 = nn.Sequential(nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block1(x)\n",
        "        out = self.pool(out)\n",
        "        out = self.conv_block2(out)\n",
        "        out = self.pool(out)\n",
        "        out = self.conv_block3(out)\n",
        "        out = self.conv_block4(out)\n",
        "        out = self.conv_block5(out)\n",
        "        out = self.pool(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VHx6guLr5rIy",
      "metadata": {
        "id": "VHx6guLr5rIy"
      },
      "source": [
        "*   `Dropout`ï¼šè®©ä¸€éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºä¸º0. ç›®å‰è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªæ­£åˆ™é¡¹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚ç›®å‰ç”±äºå¾ˆå°‘ä½¿ç”¨å…¨è¿æ¥äº†ï¼Œæ‰€ä»¥`Dropout`ä¸æ˜¯é‚£ä¹ˆé‡è¦äº†ã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KWhJKo41uP4S",
      "metadata": {
        "id": "KWhJKo41uP4S"
      },
      "source": [
        "### 5. è®¾ç½®è¶…å‚æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ON-DljTnuSjk",
      "metadata": {
        "id": "ON-DljTnuSjk"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "epochs = 20\n",
        "learning_rate = 0.005\n",
        "\n",
        "model = AlexNet(num_classes).to(device)\n",
        "\n",
        "# è®¾ç½®æŸå¤±å‡½æ•°\n",
        "cost = nn.CrossEntropyLoss()\n",
        "\n",
        "# è®¾ç½®ä¼˜åŒ–å™¨\n",
        "optimizer = torch.optim.SGD(model.parameters(),     # SGD éšæœºæ¢¯åº¦ä¸‹é™\n",
        "                            lr = learning_rate,\n",
        "                            weight_decay = 0.005,   # æ­£åˆ™åŒ–é¡¹çš„æƒé‡æ˜¯ 0.005\n",
        "                            momentum = 0.9)\n",
        "\n",
        "# ä¸€ epoch è®­ç»ƒçš„æ€» step æ•°\n",
        "train_step = len(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nFpMTApn61p4",
      "metadata": {
        "id": "nFpMTApn61p4"
      },
      "source": [
        "\n",
        "*   `weight_decay`ï¼šæ˜¯L2çš„æ­£åˆ™é¡¹ã€‚\n",
        "*   **æ­£åˆ™åŒ–**ï¼šåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæ­£åˆ™åŒ–æ˜¯ä¸€ç§ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ã€‚é™åˆ¶æ¨¡å‹çš„å¤æ‚æ€§ï¼Œé˜²æ­¢æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°è¿‡äºæ•æ„Ÿï¼Œä»è€Œæé«˜å…¶åœ¨æœªçŸ¥æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ AlexNet æå‡ºé‚£å¹´ï¼ˆ2012ï¼‰ï¼Œäººä»¬æ™®éè®¤ä¸ºæ­£åˆ™åŒ–å¯¹è§£å†³æ¨¡å‹è¿‡æ‹Ÿåˆé—®é¢˜æ˜¯å¾ˆé‡è¦çš„ã€‚ä½†åœ¨åæœŸï¼Œè¿™ä¸ªè§‚ç‚¹è¢«æ¨ç¿»äº†ã€‚å–è€Œä»£ä¹‹çš„æ˜¯ï¼Œç½‘ç»œçš„è®¾è®¡å¯¹é˜²æ­¢è¿‡æ‹Ÿåˆæ›´é‡è¦çš„ã€‚\n",
        "*   `momentum`ï¼šåŠ¨é‡æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ä¸­å¸¸ç”¨çš„æŠ€æœ¯ï¼Œé€šå¸¸ä¸éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ç»“åˆä½¿ç”¨ï¼Œç”¨äºåŠ é€Ÿæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®ƒçš„åŠŸèƒ½æ˜¯**é¿å…å› ä¸‹é™æ›²çº¿ä¸å¹³æ»‘è€Œè½å…¥å±€éƒ¨æœ€ä¼˜è§£ä¸­ã€‚**åŠ¨é‡çš„å¼•å…¥ä¸»è¦æ˜¯ä¸ºäº†è§£å†³éšæœºæ¢¯åº¦ä¸‹é™çš„ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚åœ¨æ¢¯åº¦æ›´æ–°ä¸­å­˜åœ¨çš„éœ‡è¡å’Œæ”¶æ•›é€Ÿåº¦æ…¢çš„é—®é¢˜ã€‚åŠ¨é‡ç®—æ³•å¼•å…¥äº†ä¸€ä¸ªæŒ‡æ•°è¡°å‡çš„ç´¯ç§¯å˜é‡ï¼Œç”¨æ¥æŒç»­è·Ÿè¸ªæ¢¯åº¦çš„å†å²ä¿¡æ¯ã€‚è¿™ä¸ªç´¯ç§¯å˜é‡å°±æ˜¯åŠ¨é‡ã€‚åŠ¨é‡åœ¨æ›´æ–°å‚æ•°æ—¶ä¸ä»…è€ƒè™‘å½“å‰æ¢¯åº¦ï¼Œè¿˜è€ƒè™‘äº†ä¹‹å‰æ¢¯åº¦çš„æ–¹å‘ã€‚è¿™æœ‰åŠ©äºå¹³æ»‘æ›´æ–°è¿‡ç¨‹ï¼Œå‡å°‘å‚æ•°æ›´æ–°çš„éœ‡è¡ï¼Œæé«˜æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§å’Œé€Ÿåº¦ã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3RCizqAeUbc",
      "metadata": {
        "id": "c3RCizqAeUbc"
      },
      "source": [
        "### 6. è®­ç»ƒå’ŒéªŒè¯\n",
        "è®­ç»ƒéœ€è¦ 2 ä¸ªå¾ªç¯çš„åµŒå¥—ï¼šå¤–éƒ¨å¾ªç¯ç”¨äºå¾ªç¯ epoch ï¼›å†…éƒ¨å¾ªç¯ç”¨äºå¾ªç¯æ¯ä¸ª epoch ä¸­çš„æ¯ä¸ª step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W1fbJ6VaeYzD",
      "metadata": {
        "id": "W1fbJ6VaeYzD"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = cost(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(\"Epoch [{}/{}], Step [{}/{}], Loss:{:.4f}\".format(\n",
        "      epoch+1, epochs, i+1, train_step, loss.item()))   # lossæ˜¯å¼ é‡ï¼Œéœ€è¦.item()è½¬ä¸ºæµ®ç‚¹å‹\n",
        "\n",
        "  # ä¸€ä¸ªepochå®Œæˆä¹‹åï¼Œè¿›å…¥éªŒè¯\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in val_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      # del images, labels, outputs   # åˆ é™¤å˜é‡ä»¥é‡Šæ”¾å†…å­˜\n",
        "\n",
        "    # è¾“å‡ºéªŒè¯ç»“æœ\n",
        "    print(\"Accuracy on validation: {} %\".format(100 * (correct / total)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y6JuJpFFyvcG",
      "metadata": {
        "id": "Y6JuJpFFyvcG"
      },
      "source": [
        "\n",
        "*   `torch.max(outputs.data, 1)`è¿”å›æ¯ä¸€è¡Œçš„æœ€å¤§å€¼ä»¥åŠè¿™äº›æœ€å¤§å€¼æ‰€åœ¨çš„ç´¢å¼•ã€‚ç¬¬ä¸€ä¸ªè¿”å›å€¼ï¼ˆ`_`ï¼‰æ˜¯æœ€å¤§å€¼ï¼Œç¬¬äºŒä¸ªè¿”å›å€¼ï¼ˆ`predicted`ï¼‰æ˜¯æœ€å¤§å€¼çš„ç´¢å¼•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªå…³å¿ƒç´¢å¼•ï¼Œå› ä¸ºå®ƒè¡¨ç¤ºäº†æ¨¡å‹çš„é¢„æµ‹ç±»åˆ«ã€‚\n",
        "*   `labels.size(0)`è¿”å›çš„æ˜¯å½“å‰æ‰¹æ¬¡ä¸­æ ‡ç­¾çš„æ•°é‡ã€‚\n",
        "*   `del images, labels, outputs` æ˜¯æ‰‹åŠ¨åˆ é™¤å˜é‡ä»¥é‡Šæ”¾å†…å­˜ã€‚åœ¨Pythonä¸­ï¼Œè¿™é€šå¸¸æ˜¯ä¸å¿…è¦çš„ï¼Œå› ä¸ºPythonçš„åƒåœ¾å›æ”¶å™¨ä¼šè‡ªåŠ¨å¤„ç†ä¸å†ä½¿ç”¨çš„å¯¹è±¡ã€‚\n",
        "*   `for images, labels in val_loader:`ä¸éœ€è¦`numerate(val_loader)`æ˜¯å› ä¸ºéªŒè¯é˜¶æ®µä¸éœ€è¦ç´¢å¼•ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tw9P9qts9T1d",
      "metadata": {
        "id": "tw9P9qts9T1d"
      },
      "source": [
        "### 7. æµ‹è¯•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KKPaQr8a9Zia",
      "metadata": {
        "id": "KKPaQr8a9Zia"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    # del images, labels, outputs   # åˆ é™¤å˜é‡ä»¥é‡Šæ”¾å†…å­˜\n",
        "\n",
        "  # è¾“å‡ºæµ‹è¯•ç»“æœ\n",
        "  print(\"Accuracy on test: {} %\".format(100 * (correct / total)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
